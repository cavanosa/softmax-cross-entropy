{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_softmax_cross_entropy_parte02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVeT1/k01inJok3JDE33W0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAy3cz6bhxsg"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6u8RjFnh7rB"
      },
      "source": [
        "flowers = load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm1W28KYiCjE"
      },
      "source": [
        "class NeuralNet():\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
        "    self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
        "\n",
        "    self.b1 = np.zeros((1, self.hidden_size))\n",
        "    self.b2 = np.zeros((1, self.output_size))\n",
        "\n",
        "  def forward(self, X):\n",
        "    self.z1 = np.dot(X, self.W1) + self.b1\n",
        "    self.a1 = self.relu(self.z1)\n",
        "    self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "    self.output = self.softmax(self.z2)\n",
        "    return self.output\n",
        "\n",
        "  def relu(self, x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "  def softmax(self, x):\n",
        "    exp = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "    return exp\n",
        "\n",
        "  def cross_entropy(self, output, y):\n",
        "    target = self.one_hot(y)\n",
        "    loss = - np.mean(target * np.log(output + 1e-8))\n",
        "    return loss\n",
        "\n",
        "  def one_hot(self, y):\n",
        "    one_hot_array = np.zeros((y.size, y.max() + 1))\n",
        "    one_hot_array[np.arange(y.size), y] = 1\n",
        "    return one_hot_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoiYZHmCko1q"
      },
      "source": [
        "X = flowers.data\n",
        "y = flowers.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpUPeKN4k2fL"
      },
      "source": [
        "nn = NeuralNet(X.shape[1], 8, y.max() + 1)\n",
        "output = nn.forward(X)\n",
        "nn.one_hot(y)\n",
        "nn.cross_entropy(output, nn.one_hot(y))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}